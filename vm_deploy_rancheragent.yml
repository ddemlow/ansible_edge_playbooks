---
- name: Simple vm deploy docker single node  # edit vmname variable - use -l filter to specify cluster vs. full inventory
  hosts: all
  vars:
    vmname: rke-
    image_url: https://cloud-images.ubuntu.com/noble/current/noble-server-cloudimg-amd64.img #"https://cloud-images.ubuntu.com/jammy/current/jammy-server-cloudimg-amd64.img"
    image_path: "/Users/davedemlow/tmp/" # ~/tmp/" #path to download file
    url2template_image_url: "{{ image_url }}"
    url2template_machine_type: vTPM+UEFI
    url2template_operating_system: os_other
    url2template_vm_name: "{{ vm_name | default(image_url | split('/') | last) }}"
  connection: local
  gather_facts: false
  strategy: host_pinned # free  #allows each cluster to start next task before all clusters have finished current task
  environment:  # if set here - hypercore modules will automatically use this for each remote cluster - avoiding need to specify cluster_instance for each test
    SC_HOST: "https://{{ inventory_hostname }}"
    SC_USERNAME: "{{ scale_user | default('admin') }}"
    SC_PASSWORD: "{{ scale_pass | default('admin') }}"
#    SC_TIMEOUT: 15
  roles:
    
    - url2template  #uses new collection role

  pre_tasks:
#  see if uploading- exists
    - name: Get Information About the uploaded Virtual Disk in HyperCore
      scale_computing.hypercore.virtual_disk_info:
      register: diskmedia

    # - name: output virtual disk media
    #   debug:
    #     msg: "{{ diskmedia }}"

    - name: Collect disks starting with 'uploading-'
      set_fact:
        disks_to_delete: "{{ diskmedia.records | selectattr('name', 'match', '^uploading-.*') | map(attribute='name') | list }}"

    # - name: Debug - Show disks to delete
    #   debug:
    #     msg: "{{ disks_to_delete }}"

    - name: Delete uploading- virtual disks
      scale_computing.hypercore.virtual_disk:
        name: "{{ item }}"
        state: absent
      loop: "{{ disks_to_delete }}"
      when: disks_to_delete is defined and disks_to_delete | length > 0
      register: deleted

    - name: Collect disks starting with 'converting-'
      set_fact:
        disks_to_delete: "{{ diskmedia.records | selectattr('name', 'match', '^converting-.*') | map(attribute='name') | list }}"

    # - name: Debug - Show disks to delete
    #   debug:
    #     msg: "{{ disks_to_delete }}"

    - name: Delete converting- virtual disks
      scale_computing.hypercore.virtual_disk:
        name: "{{ item }}"
        state: absent
      loop: "{{ disks_to_delete }}"
      when: disks_to_delete is defined and disks_to_delete | length > 0
      register: deleted

    # - name: Delete converting- virtual disks
    #   scale_computing.hypercore.virtual_disk:
    #     name: "{{ item }}"
    #     state: absent
    #   loop: "{{ disks_to_delete }}"
    #   when: disks_to_delete is defined and disks_to_delete | length > 0
    #   register: deleted

    # - name: Delete uploaded virtual dsk
    #   scale_computing.hypercore.virtual_disk:
    #     name:  "{{ url2template_image_url | split('/') | last }}"
    #     state: absent
    #   register: deleted

    # - name: delete Template
    #   scale_computing.hypercore.vm:
    #     vm_name: "{{ url2template_vm_name }}"
    #     state: absent

  tasks:
    - name: Generate 5 digit random character password using lower case ans set as variable named ran5
      ansible.builtin.set_fact:
        ran5: "{{ lookup('password', '/dev/null chars=ascii_lowercase,digits length=5') }}"

    # - name: Delete existing uploading-"{{ url2template_vm_name }}" virtual disk # recovers from any previous failed upload 
    #   scale_computing.hypercore.virtual_disk:
    #     name: "uploading-{{ url2template_vm_name }}"
    #     state: absent
    #   register: deleted

    - name: Clone and configure ad hoc "{{ vmname }} {{ran5}}"
      scale_computing.hypercore.vm_clone:
        vm_name: "{{ vmname }}{{ ran5 }}"
        source_vm_name: "{{ url2template_vm_name }}"
        tags:
          - appdeploy
        cloud_init:
          user_data: |
            #cloud-config
            password: "password"
            chpasswd: { expire: False }
            ssh_pwauth: True
            ssh_authorized_keys: # Add your ssh public key for publickey authentication
              - ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIDihWWhjoPj8KVLtdLDwNJQ71zi9An0iUFjefRWu2Eju ddemlow@scalecomputing.com
            disable_root: false
            ssh_import_id:  gh:ddemlow
            package_update: true
            package_upgrade: false
            packages: [qemu-guest-agent, snapd ]
            bootcmd:
              - [ sh, -c, 'sudo echo GRUB_CMDLINE_LINUX="nomodeset" >> /etc/default/grub' ]
              - [ sh, -c, 'sudo echo GRUB_GFXPAYLOAD_LINUX="1024x768" >> /etc/default/grub' ]
              - [ sh, -c, 'sudo echo GRUB_DISABLE_LINUX_UUID=true >> /etc/default/grub' ]
              - [ sh, -c, 'sudo update-grub' ]
            runcmd:
              - [ systemctl, restart, --no-block, qemu-guest-agent ]
              - # rancher registration curl command here "curl --insecure -fL https://rancherip.sslip.io/system-agent-install.sh | sudo  sh -s - --server https://rancherip.sslip.io --label 'cattle.io/os=linux' --token xxxx --ca-checksum 201d953e9eaf37618c62fe24d8264b4a7a00b8172348f42b43ebad430867009e --etcd --controlplane --worker" 
              - [ systemctl, restart, --no-block, qemu-guest-agent ]
              - snap install kubectl --classic
            write_files:
            - path: /etc/systemd/system/getty@tty1.service.d/override.conf
              content: |
                [Service]
                # Override getty@.service for tty1 to enable autologin
                ExecStart=
                ExecStart=-/sbin/agetty --autologin ubuntu --noclear %I $TERM
              permissions: '0644'
            final_message: |
              cloud-init has finished
              version: $version
              timestamp: $timestamp
              datasource: $datasource
              uptime: $uptime

          meta_data: |
            dsmode: local
            local-hostname: "{{ vmname }}{{ ran5 }}"

    - name: Disk desired configuration for "{{ vmname }}
      scale_computing.hypercore.vm_disk:
        vm_name: "{{ vmname }}{{ ran5 }}"
        items:
          - disk_slot: 1
            type: virtio_disk
            size: "{{ '300 GB' | human_to_bytes }}" # 50GB | human to bytes results in 53.7GB VSD in Hypercore
        state: present

    - name: Vm desired configuration and state for "{{ vmname }}"
      scale_computing.hypercore.vm_params:
        vm_name: "{{ vmname }}{{ ran5 }}"
        memory: "{{ '9 GB' | human_to_bytes }}"
        description:
        tags:
          - appdeploy
#          - SCALEGPUL424Q
          - ansible_group___docker
          - SERIAL
        vcpu: 6
        power_state: start
